{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b259b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 32 features\n",
      "Sample features: ['client_code', 'name', 'status', 'age', 'city', 'avg_monthly_balance_KZT', 'date', 'data_source', 'type', 'category']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_code</th>\n",
       "      <th>name</th>\n",
       "      <th>status</th>\n",
       "      <th>age</th>\n",
       "      <th>city</th>\n",
       "      <th>avg_monthly_balance_KZT</th>\n",
       "      <th>date</th>\n",
       "      <th>data_source</th>\n",
       "      <th>type</th>\n",
       "      <th>category</th>\n",
       "      <th>...</th>\n",
       "      <th>is_outflow</th>\n",
       "      <th>outflow_count</th>\n",
       "      <th>outflow_amount_sum</th>\n",
       "      <th>accumulation_ratio</th>\n",
       "      <th>net_flow</th>\n",
       "      <th>is_target_spending</th>\n",
       "      <th>spending_frequency</th>\n",
       "      <th>spending_amount_median</th>\n",
       "      <th>spending_amount_std</th>\n",
       "      <th>spending_consistency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Айгерим</td>\n",
       "      <td>Зарплатный клиент</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Алматы</td>\n",
       "      <td>92643.0</td>\n",
       "      <td>2025-06-02 12:10:21</td>\n",
       "      <td>transaction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Продукты питания</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>1164024.65</td>\n",
       "      <td>1.610867</td>\n",
       "      <td>711065.99</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>8937.45</td>\n",
       "      <td>6000.022167</td>\n",
       "      <td>0.598351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Айгерим</td>\n",
       "      <td>Зарплатный клиент</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Алматы</td>\n",
       "      <td>92643.0</td>\n",
       "      <td>2025-06-04 12:10:35</td>\n",
       "      <td>transaction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Продукты питания</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>1164024.65</td>\n",
       "      <td>1.610867</td>\n",
       "      <td>711065.99</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>8937.45</td>\n",
       "      <td>6000.022167</td>\n",
       "      <td>0.598351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Айгерим</td>\n",
       "      <td>Зарплатный клиент</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Алматы</td>\n",
       "      <td>92643.0</td>\n",
       "      <td>2025-06-04 17:00:58</td>\n",
       "      <td>transaction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кафе и рестораны</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>1164024.65</td>\n",
       "      <td>1.610867</td>\n",
       "      <td>711065.99</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>8937.45</td>\n",
       "      <td>6000.022167</td>\n",
       "      <td>0.598351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Айгерим</td>\n",
       "      <td>Зарплатный клиент</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Алматы</td>\n",
       "      <td>92643.0</td>\n",
       "      <td>2025-06-04 17:40:05</td>\n",
       "      <td>transaction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кафе и рестораны</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>1164024.65</td>\n",
       "      <td>1.610867</td>\n",
       "      <td>711065.99</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>8937.45</td>\n",
       "      <td>6000.022167</td>\n",
       "      <td>0.598351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Айгерим</td>\n",
       "      <td>Зарплатный клиент</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Алматы</td>\n",
       "      <td>92643.0</td>\n",
       "      <td>2025-06-04 19:00:08</td>\n",
       "      <td>transaction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кафе и рестораны</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>1164024.65</td>\n",
       "      <td>1.610867</td>\n",
       "      <td>711065.99</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>8937.45</td>\n",
       "      <td>6000.022167</td>\n",
       "      <td>0.598351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6375</th>\n",
       "      <td>60</td>\n",
       "      <td>Ермек</td>\n",
       "      <td>Зарплатный клиент</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кызылорда</td>\n",
       "      <td>48779.0</td>\n",
       "      <td>2025-08-31 10:00:00</td>\n",
       "      <td>transaction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кафе и рестораны</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1262660.64</td>\n",
       "      <td>1.151359</td>\n",
       "      <td>191116.65</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>8862.35</td>\n",
       "      <td>5894.889440</td>\n",
       "      <td>0.600570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6376</th>\n",
       "      <td>60</td>\n",
       "      <td>Ермек</td>\n",
       "      <td>Зарплатный клиент</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кызылорда</td>\n",
       "      <td>48779.0</td>\n",
       "      <td>2025-08-31 11:20:39</td>\n",
       "      <td>transaction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кафе и рестораны</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1262660.64</td>\n",
       "      <td>1.151359</td>\n",
       "      <td>191116.65</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>8862.35</td>\n",
       "      <td>5894.889440</td>\n",
       "      <td>0.600570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6377</th>\n",
       "      <td>60</td>\n",
       "      <td>Ермек</td>\n",
       "      <td>Зарплатный клиент</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кызылорда</td>\n",
       "      <td>48779.0</td>\n",
       "      <td>2025-08-31 12:10:15</td>\n",
       "      <td>transaction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Продукты питания</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1262660.64</td>\n",
       "      <td>1.151359</td>\n",
       "      <td>191116.65</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>8862.35</td>\n",
       "      <td>5894.889440</td>\n",
       "      <td>0.600570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6378</th>\n",
       "      <td>60</td>\n",
       "      <td>Ермек</td>\n",
       "      <td>Зарплатный клиент</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кызылорда</td>\n",
       "      <td>48779.0</td>\n",
       "      <td>2025-08-31 17:00:58</td>\n",
       "      <td>transaction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кафе и рестораны</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1262660.64</td>\n",
       "      <td>1.151359</td>\n",
       "      <td>191116.65</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>8862.35</td>\n",
       "      <td>5894.889440</td>\n",
       "      <td>0.600570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6379</th>\n",
       "      <td>60</td>\n",
       "      <td>Ермек</td>\n",
       "      <td>Зарплатный клиент</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кызылорда</td>\n",
       "      <td>48779.0</td>\n",
       "      <td>2025-08-31 19:00:50</td>\n",
       "      <td>transaction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кафе и рестораны</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1262660.64</td>\n",
       "      <td>1.151359</td>\n",
       "      <td>191116.65</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>8862.35</td>\n",
       "      <td>5894.889440</td>\n",
       "      <td>0.600570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6380 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      client_code     name             status  age       city  \\\n",
       "0               1  Айгерим  Зарплатный клиент  NaN     Алматы   \n",
       "1               1  Айгерим  Зарплатный клиент  NaN     Алматы   \n",
       "2               1  Айгерим  Зарплатный клиент  NaN     Алматы   \n",
       "3               1  Айгерим  Зарплатный клиент  NaN     Алматы   \n",
       "4               1  Айгерим  Зарплатный клиент  NaN     Алматы   \n",
       "...           ...      ...                ...  ...        ...   \n",
       "6375           60    Ермек  Зарплатный клиент  NaN  Кызылорда   \n",
       "6376           60    Ермек  Зарплатный клиент  NaN  Кызылорда   \n",
       "6377           60    Ермек  Зарплатный клиент  NaN  Кызылорда   \n",
       "6378           60    Ермек  Зарплатный клиент  NaN  Кызылорда   \n",
       "6379           60    Ермек  Зарплатный клиент  NaN  Кызылорда   \n",
       "\n",
       "      avg_monthly_balance_KZT                date  data_source type  \\\n",
       "0                     92643.0 2025-06-02 12:10:21  transaction  NaN   \n",
       "1                     92643.0 2025-06-04 12:10:35  transaction  NaN   \n",
       "2                     92643.0 2025-06-04 17:00:58  transaction  NaN   \n",
       "3                     92643.0 2025-06-04 17:40:05  transaction  NaN   \n",
       "4                     92643.0 2025-06-04 19:00:08  transaction  NaN   \n",
       "...                       ...                 ...          ...  ...   \n",
       "6375                  48779.0 2025-08-31 10:00:00  transaction  NaN   \n",
       "6376                  48779.0 2025-08-31 11:20:39  transaction  NaN   \n",
       "6377                  48779.0 2025-08-31 12:10:15  transaction  NaN   \n",
       "6378                  48779.0 2025-08-31 17:00:58  transaction  NaN   \n",
       "6379                  48779.0 2025-08-31 19:00:50  transaction  NaN   \n",
       "\n",
       "              category  ... is_outflow  outflow_count outflow_amount_sum  \\\n",
       "0     Продукты питания  ...          1            112         1164024.65   \n",
       "1     Продукты питания  ...          1            112         1164024.65   \n",
       "2     Кафе и рестораны  ...          1            112         1164024.65   \n",
       "3     Кафе и рестораны  ...          1            112         1164024.65   \n",
       "4     Кафе и рестораны  ...          1            112         1164024.65   \n",
       "...                ...  ...        ...            ...                ...   \n",
       "6375  Кафе и рестораны  ...          1            123         1262660.64   \n",
       "6376  Кафе и рестораны  ...          1            123         1262660.64   \n",
       "6377  Продукты питания  ...          1            123         1262660.64   \n",
       "6378  Кафе и рестораны  ...          1            123         1262660.64   \n",
       "6379  Кафе и рестораны  ...          1            123         1262660.64   \n",
       "\n",
       "      accumulation_ratio   net_flow  is_target_spending  spending_frequency  \\\n",
       "0               1.610867  711065.99                   1                 112   \n",
       "1               1.610867  711065.99                   1                 112   \n",
       "2               1.610867  711065.99                   1                 112   \n",
       "3               1.610867  711065.99                   1                 112   \n",
       "4               1.610867  711065.99                   1                 112   \n",
       "...                  ...        ...                 ...                 ...   \n",
       "6375            1.151359  191116.65                   1                 123   \n",
       "6376            1.151359  191116.65                   1                 123   \n",
       "6377            1.151359  191116.65                   1                 123   \n",
       "6378            1.151359  191116.65                   1                 123   \n",
       "6379            1.151359  191116.65                   1                 123   \n",
       "\n",
       "      spending_amount_median  spending_amount_std  spending_consistency  \n",
       "0                    8937.45          6000.022167              0.598351  \n",
       "1                    8937.45          6000.022167              0.598351  \n",
       "2                    8937.45          6000.022167              0.598351  \n",
       "3                    8937.45          6000.022167              0.598351  \n",
       "4                    8937.45          6000.022167              0.598351  \n",
       "...                      ...                  ...                   ...  \n",
       "6375                 8862.35          5894.889440              0.600570  \n",
       "6376                 8862.35          5894.889440              0.600570  \n",
       "6377                 8862.35          5894.889440              0.600570  \n",
       "6378                 8862.35          5894.889440              0.600570  \n",
       "6379                 8862.35          5894.889440              0.600570  \n",
       "\n",
       "[6380 rows x 32 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../separate_dfs/deposit_accumulative_data.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Fill balance data\n",
    "df['avg_monthly_balance_KZT'] = df.groupby('client_code')['avg_monthly_balance_KZT'].transform('first')\n",
    "\n",
    "# 1. Balance features\n",
    "df['balance_median'] = df.groupby('client_code')['avg_monthly_balance_KZT'].transform('median')\n",
    "df['balance_cv'] = df.groupby('client_code')['avg_monthly_balance_KZT'].transform(lambda x: x.std() / x.mean() if x.mean() > 0 else 0)\n",
    "\n",
    "# 2. Deposit features\n",
    "deposit_types = ['salary_in', 'stipend_in', 'family_in', 'deposit_topup_out', 'cashback_in', 'refund_in']\n",
    "df['is_deposit'] = df['type'].isin(deposit_types).astype(int)\n",
    "df['deposit_count'] = df.groupby('client_code')['is_deposit'].transform('sum')\n",
    "\n",
    "# Calculate deposit regularity at client level (not row level)\n",
    "deposit_dates = df[df['is_deposit']==1].groupby('client_code')['date'].apply(lambda x: x.diff().dt.days.std()).fillna(0)\n",
    "df['deposit_regularity'] = df['client_code'].map(deposit_dates).fillna(0)\n",
    "\n",
    "# 3. Inflow features\n",
    "df['is_inflow'] = ((df['data_source'] == 'transfer') & (df['direction'] == 'in')).astype(int)\n",
    "df['inflow_count'] = df.groupby('client_code')['is_inflow'].transform('sum')\n",
    "\n",
    "# Calculate inflow sums and averages at client level\n",
    "inflow_sums = df[df['is_inflow']==1].groupby('client_code')['amount'].sum().fillna(0)\n",
    "inflow_means = df[df['is_inflow']==1].groupby('client_code')['amount'].mean().fillna(0)\n",
    "df['inflow_amount_sum'] = df['client_code'].map(inflow_sums).fillna(0)\n",
    "df['avg_inflow_amount'] = df['client_code'].map(inflow_means).fillna(0)\n",
    "\n",
    "# 4. Outflow features\n",
    "df['is_outflow'] = ((df['data_source'] == 'transaction') | ((df['data_source'] == 'transfer') & (df['direction'] == 'out'))).astype(int)\n",
    "df['outflow_count'] = df.groupby('client_code')['is_outflow'].transform('sum')\n",
    "\n",
    "outflow_sums = df[df['is_outflow']==1].groupby('client_code')['amount'].sum().fillna(0)\n",
    "df['outflow_amount_sum'] = df['client_code'].map(outflow_sums).fillna(0)\n",
    "df['accumulation_ratio'] = df['inflow_amount_sum'] / (df['outflow_amount_sum'] + 1)\n",
    "df['net_flow'] = df['inflow_amount_sum'] - df['outflow_amount_sum']\n",
    "\n",
    "# 5. Spending features\n",
    "spending_categories = ['Продукты питания', 'Кафе и рестораны', 'Одежда и обувь', 'Спорт', 'Книги', 'Косметика и Парфюмерия']\n",
    "df['is_target_spending'] = df['category'].isin(spending_categories).astype(int)\n",
    "df['spending_frequency'] = df.groupby('client_code')['is_target_spending'].transform('sum')\n",
    "\n",
    "spending_medians = df[df['is_target_spending']==1].groupby('client_code')['amount'].median().fillna(0)\n",
    "spending_stds = df[df['is_target_spending']==1].groupby('client_code')['amount'].std().fillna(0)\n",
    "df['spending_amount_median'] = df['client_code'].map(spending_medians).fillna(0)\n",
    "df['spending_amount_std'] = df['client_code'].map(spending_stds).fillna(0)\n",
    "df['spending_consistency'] = 1 / (1 + df['spending_amount_std'] / (df['spending_amount_median'] + 1))\n",
    "\n",
    "print(f\"Created {len(df.columns)} features\")\n",
    "print(f\"Sample features: {df.columns.tolist()[:10]}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "394f8569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating 6380 rows to client level...\n",
      "Created 44 client profiles\n",
      "Calculating data-driven feature importance...\n",
      "\n",
      "Top 5 Most Important Features:\n",
      "  avg_inflow_amount: 0.067\n",
      "  inflow_count: 0.066\n",
      "  deposit_count: 0.062\n",
      "  inflow_amount_sum: 0.061\n",
      "  savings_rate: 0.060\n",
      "\n",
      "Calculating client scores...\n",
      "\n",
      "Score Distribution:\n",
      "recommendation_tier\n",
      "High         24\n",
      "Medium       19\n",
      "Very High     1\n",
      "Low           0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 10 Clients:\n",
      "    client_code  composite_score  score_percentile recommendation_tier\n",
      "6            12         0.715894          1.000000           Very High\n",
      "38           49         0.689498          0.977273                High\n",
      "4             8         0.641200          0.954545                High\n",
      "5            10         0.624162          0.931818                High\n",
      "28           37         0.614453          0.909091                High\n",
      "23           30         0.611974          0.886364                High\n",
      "0             1         0.600694          0.863636                High\n",
      "3             6         0.599370          0.840909                High\n",
      "41           56         0.577691          0.818182                High\n",
      "18           25         0.570570          0.795455                High\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import percentileofscore\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "class DataDrivenScoringFramework:\n",
    "    def __init__(self, product_name):\n",
    "        self.product_name = product_name\n",
    "        self.feature_importance = {}\n",
    "        self.client_features = None\n",
    "        self.scores = None\n",
    "    \n",
    "    def aggregate_to_client_level(self, df):\n",
    "        \"\"\"Aggregate with only columns that exist\"\"\"\n",
    "        \n",
    "        # Define aggregation only for columns that exist\n",
    "        available_cols = df.columns.tolist()\n",
    "        \n",
    "        agg_dict = {}\n",
    "        \n",
    "        # Map of desired columns and their aggregation\n",
    "        desired_aggs = {\n",
    "            'avg_monthly_balance_KZT': 'first',\n",
    "            'balance_median': 'first',\n",
    "            'balance_cv': 'first',\n",
    "            'deposit_count': 'first',\n",
    "            'deposit_regularity': 'first',\n",
    "            'inflow_count': 'first',\n",
    "            'inflow_amount_sum': 'first',\n",
    "            'avg_inflow_amount': 'first',\n",
    "            'outflow_count': 'first',\n",
    "            'outflow_amount_sum': 'first',\n",
    "            'accumulation_ratio': 'first',\n",
    "            'net_flow': 'first',\n",
    "            'spending_frequency': 'first',\n",
    "            'spending_amount_median': 'first',\n",
    "            'spending_amount_std': 'first',\n",
    "            'spending_consistency': 'first'\n",
    "        }\n",
    "        \n",
    "        # Only include columns that exist\n",
    "        for col, agg_func in desired_aggs.items():\n",
    "            if col in available_cols:\n",
    "                agg_dict[col] = agg_func\n",
    "        \n",
    "        client_df = df.groupby('client_code').agg(agg_dict).reset_index()\n",
    "        \n",
    "        # Add derived features\n",
    "        if 'inflow_count' in client_df.columns and 'outflow_count' in client_df.columns:\n",
    "            client_df['inflow_regularity'] = client_df['inflow_count'] / (client_df['outflow_count'] + 1)\n",
    "        \n",
    "        if 'net_flow' in client_df.columns and 'inflow_amount_sum' in client_df.columns:\n",
    "            client_df['savings_rate'] = client_df['net_flow'] / (client_df['inflow_amount_sum'] + 1)\n",
    "        \n",
    "        if 'spending_frequency' in client_df.columns and 'outflow_count' in client_df.columns:\n",
    "            client_df['transaction_diversity'] = client_df['spending_frequency'] / (client_df['outflow_count'] + 1)\n",
    "        \n",
    "        return client_df\n",
    "    \n",
    "    def calculate_feature_importance(self, client_df):\n",
    "        \"\"\"Calculate feature importance using multiple methods\"\"\"\n",
    "        \n",
    "        feature_cols = [col for col in client_df.columns if col != 'client_code']\n",
    "        \n",
    "        # Prepare data\n",
    "        X = client_df[feature_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "        X_scaled = StandardScaler().fit_transform(X)\n",
    "        \n",
    "        importance_scores = {}\n",
    "        \n",
    "        # Method 1: PCA-based importance\n",
    "        n_components = min(5, len(feature_cols), len(client_df) - 1)\n",
    "        if n_components > 0:\n",
    "            pca = PCA(n_components=n_components)\n",
    "            pca.fit(X_scaled)\n",
    "            \n",
    "            # Get loadings for components that explain most variance\n",
    "            n_important_components = min(3, n_components)\n",
    "            loadings = np.abs(pca.components_[:n_important_components, :]).mean(axis=0)\n",
    "            pca_importance = dict(zip(feature_cols, loadings / (loadings.sum() + 1e-10)))\n",
    "        else:\n",
    "            pca_importance = {col: 1/len(feature_cols) for col in feature_cols}\n",
    "        \n",
    "        # Method 2: Variance-based importance\n",
    "        variances = X.var()\n",
    "        if variances.sum() > 0:\n",
    "            variance_scores = 1 - np.abs(variances - variances.median()) / (variances.max() + 1e-10)\n",
    "            variance_importance = dict(zip(feature_cols, variance_scores / (variance_scores.sum() + 1e-10)))\n",
    "        else:\n",
    "            variance_importance = {col: 1/len(feature_cols) for col in feature_cols}\n",
    "        \n",
    "        # Method 3: Correlation uniqueness\n",
    "        if len(client_df) > 1:\n",
    "            corr_matrix = X.corr().abs()\n",
    "            np.fill_diagonal(corr_matrix.values, 0)\n",
    "            uniqueness = 1 - corr_matrix.mean(axis=0)\n",
    "            uniqueness_importance = dict(zip(feature_cols, uniqueness / (uniqueness.sum() + 1e-10)))\n",
    "        else:\n",
    "            uniqueness_importance = {col: 1/len(feature_cols) for col in feature_cols}\n",
    "        \n",
    "        # Combine methods (skip anomaly detection for stability)\n",
    "        for feature in feature_cols:\n",
    "            importance_scores[feature] = (\n",
    "                pca_importance.get(feature, 0) * 0.40 +\n",
    "                variance_importance.get(feature, 0) * 0.30 +\n",
    "                uniqueness_importance.get(feature, 0) * 0.30\n",
    "            )\n",
    "        \n",
    "        # Normalize\n",
    "        total = sum(importance_scores.values())\n",
    "        if total > 0:\n",
    "            self.feature_importance = {k: v/total for k, v in importance_scores.items()}\n",
    "        else:\n",
    "            self.feature_importance = {k: 1/len(feature_cols) for k in feature_cols}\n",
    "        \n",
    "        return self.feature_importance\n",
    "    \n",
    "    def calculate_client_scores(self, client_df):\n",
    "        \"\"\"Calculate scores using data-driven weights\"\"\"\n",
    "        \n",
    "        feature_cols = [col for col in client_df.columns if col != 'client_code']\n",
    "        \n",
    "        # Normalize features to 0-1\n",
    "        for col in feature_cols:\n",
    "            values = client_df[col].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "            if len(set(values)) > 1:  # Only normalize if there's variance\n",
    "                client_df[f'{col}_norm'] = [percentileofscore(values, x)/100 for x in values]\n",
    "            else:\n",
    "                client_df[f'{col}_norm'] = 0.5  # Default to middle if no variance\n",
    "        \n",
    "        # Calculate weighted score\n",
    "        client_df['composite_score'] = 0\n",
    "        for feature in feature_cols:\n",
    "            if f'{feature}_norm' in client_df.columns:\n",
    "                weight = self.feature_importance.get(feature, 0)\n",
    "                client_df['composite_score'] += client_df[f'{feature}_norm'] * weight\n",
    "        \n",
    "        # Ensure 0-1 range\n",
    "        client_df['composite_score'] = client_df['composite_score'].clip(0, 1)\n",
    "        \n",
    "        # Add percentile rank\n",
    "        client_df['score_percentile'] = client_df['composite_score'].rank(pct=True)\n",
    "        \n",
    "        # Recommendation tiers\n",
    "        client_df['recommendation_tier'] = pd.cut(\n",
    "            client_df['composite_score'],\n",
    "            bins=[0, 0.3, 0.5, 0.7, 1.0],\n",
    "            labels=['Low', 'Medium', 'High', 'Very High']\n",
    "        )\n",
    "        \n",
    "        return client_df\n",
    "    \n",
    "    def fit_score(self, df):\n",
    "        \"\"\"End-to-end scoring pipeline\"\"\"\n",
    "        \n",
    "        # Step 1: Aggregate\n",
    "        print(f\"\\nAggregating {len(df)} rows to client level...\")\n",
    "        client_df = self.aggregate_to_client_level(df)\n",
    "        print(f\"Created {len(client_df)} client profiles\")\n",
    "        \n",
    "        # Step 2: Calculate feature importance\n",
    "        print(\"Calculating data-driven feature importance...\")\n",
    "        self.calculate_feature_importance(client_df)\n",
    "        \n",
    "        print(\"\\nTop 5 Most Important Features:\")\n",
    "        top_features = sorted(self.feature_importance.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        for feat, importance in top_features:\n",
    "            print(f\"  {feat}: {importance:.3f}\")\n",
    "        \n",
    "        # Step 3: Calculate scores\n",
    "        print(\"\\nCalculating client scores...\")\n",
    "        client_df = self.calculate_client_scores(client_df)\n",
    "        \n",
    "        self.client_features = client_df\n",
    "        self.scores = client_df[['client_code', 'composite_score', 'score_percentile', 'recommendation_tier']]\n",
    "        \n",
    "        return self.scores\n",
    "\n",
    "# Apply the framework\n",
    "scorer = DataDrivenScoringFramework('deposit_accumulative')\n",
    "scores = scorer.fit_score(df)\n",
    "\n",
    "print(\"\\nScore Distribution:\")\n",
    "print(scores['recommendation_tier'].value_counts())\n",
    "print(\"\\nTop 10 Clients:\")\n",
    "print(scores.nlargest(10, 'composite_score'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
