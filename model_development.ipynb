{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35555e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b078e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 1: Data Audit Summary ===\n",
      "Rows: 178, Columns: 51\n",
      "Unique clients: 59\n",
      "Unique cities: 9\n",
      "Months detected: 2025-06, 2025-07, 2025-08\n",
      "Saved:\n",
      " - step1_preview.csv\n",
      " - step1_month_distribution.csv\n",
      " - step1_missing_numeric.csv\n",
      " - step1_negative_numeric.csv\n",
      " - step1_validation_report.json\n"
     ]
    }
   ],
   "source": [
    "def parse_month_to_ym(m):\n",
    "    \"\"\"Parse diverse month formats to 'YYYY-MM' or return None.\"\"\"\n",
    "    if pd.isna(m):\n",
    "        return None\n",
    "    dt = pd.to_datetime(str(m), errors=\"coerce\")\n",
    "    if pd.isna(dt):\n",
    "        return None\n",
    "    return f\"{dt.year:04d}-{dt.month:02d}\"\n",
    "\n",
    "\n",
    "def run_step1_audit(features_path: str, out_dir: str = \".\", preview_rows: int = 15) -> dict:\n",
    "    features_path = Path(features_path)\n",
    "    out_dir = Path(out_dir)\n",
    "    assert features_path.exists(), f\"File not found: {features_path}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(features_path)\n",
    "\n",
    "    # Identify numeric columns: transactions/transfers + common totals\n",
    "    num_cols = [c for c in df.columns if c.startswith((\"trs_\", \"trf_\"))]\n",
    "    for c in [\"income_total\", \"spending_total\", \"avg_monthly_balance_KZT\", \"age\"]:\n",
    "        if c in df.columns:\n",
    "            num_cols.append(c)\n",
    "    num_cols = sorted(set(num_cols))\n",
    "\n",
    "    # Month distribution\n",
    "    if \"month\" in df.columns:\n",
    "        month_parsed = df[\"month\"].apply(parse_month_to_ym)\n",
    "        month_counts = (\n",
    "            month_parsed.value_counts(dropna=False)\n",
    "            .rename_axis(\"month\")\n",
    "            .reset_index(name=\"rows\")\n",
    "            .sort_values(\"month\")\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    else:\n",
    "        month_counts = pd.DataFrame({\"month\": [\"<absent>\"], \"rows\": [len(df)]})\n",
    "\n",
    "    # Basic stats\n",
    "    n_rows = len(df)\n",
    "    n_clients = df[\"client_code\"].nunique() if \"client_code\" in df.columns else None\n",
    "    n_cities = df[\"city\"].nunique() if \"city\" in df.columns else None\n",
    "\n",
    "    # Missing values in numeric columns\n",
    "    nan_summary = (\n",
    "        df[num_cols].isna().sum().sort_values(ascending=False)\n",
    "        .rename_axis(\"column\").reset_index(name=\"n_missing\")\n",
    "    )\n",
    "\n",
    "    # Negative values in numeric columns (should be >=0 for spends/income here)\n",
    "    neg_summary = (\n",
    "        (df[num_cols] < 0).sum().sort_values(ascending=False)\n",
    "        .rename_axis(\"column\").reset_index(name=\"n_negative\")\n",
    "    )\n",
    "\n",
    "    # Totals sanity (only if columns exist)\n",
    "    totals = {\n",
    "        \"sum_spending_total\": float(df[\"spending_total\"].sum()) if \"spending_total\" in df.columns else None,\n",
    "        \"sum_income_total\": float(df[\"income_total\"].sum()) if \"income_total\" in df.columns else None,\n",
    "        \"avg_balance_mean\": float(df[\"avg_monthly_balance_KZT\"].mean()) if \"avg_monthly_balance_KZT\" in df.columns else None,\n",
    "    }\n",
    "\n",
    "    # Preview\n",
    "    preview_cols = [c for c in [\n",
    "        \"client_code\", \"name\", \"status\", \"city\", \"age\",\n",
    "        \"month\", \"avg_monthly_balance_KZT\", \"income_total\", \"spending_total\", \"product\"\n",
    "    ] if c in df.columns]\n",
    "    preview = df[preview_cols].head(preview_rows)\n",
    "\n",
    "    # ---- Save artifacts\n",
    "    preview.to_csv(out_dir / \"step1_preview.csv\", index=False)\n",
    "    month_counts.to_csv(out_dir / \"step1_month_distribution.csv\", index=False)\n",
    "    nan_summary.to_csv(out_dir / \"step1_missing_numeric.csv\", index=False)\n",
    "    neg_summary.to_csv(out_dir / \"step1_negative_numeric.csv\", index=False)\n",
    "\n",
    "    report = {\n",
    "        \"shape\": [int(df.shape[0]), int(df.shape[1])],\n",
    "        \"n_unique_clients\": int(n_clients) if n_clients is not None else None,\n",
    "        \"n_unique_cities\": int(n_cities) if n_cities is not None else None,\n",
    "        \"months_detected\": month_counts.to_dict(orient=\"records\"),\n",
    "        \"nan_top30\": nan_summary.head(30).to_dict(orient=\"records\"),\n",
    "        \"neg_top30\": neg_summary.head(30).to_dict(orient=\"records\"),\n",
    "        \"totals\": totals,\n",
    "        \"notes\": [\n",
    "            \"Ignore 'product' for labels — it's current ownership, not the optimal product.\",\n",
    "            \"Assume KZT for amounts; currency normalization can be added later if needed.\"\n",
    "        ],\n",
    "    }\n",
    "    with open(out_dir / \"step1_validation_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # ---- Console summary\n",
    "    print(\"=== STEP 1: Data Audit Summary ===\")\n",
    "    print(f\"Rows: {n_rows}, Columns: {df.shape[1]}\")\n",
    "    if n_clients is not None:\n",
    "        print(f\"Unique clients: {n_clients}\")\n",
    "    if n_cities is not None:\n",
    "        print(f\"Unique cities: {n_cities}\")\n",
    "    if \"month\" in df.columns:\n",
    "        print(\"Months detected:\", \", \".join(month_counts['month'].astype(str).tolist()))\n",
    "    print(\"Saved:\")\n",
    "    print(\" - step1_preview.csv\")\n",
    "    print(\" - step1_month_distribution.csv\")\n",
    "    print(\" - step1_missing_numeric.csv\")\n",
    "    print(\" - step1_negative_numeric.csv\")\n",
    "    print(\" - step1_validation_report.json\")\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example run:\n",
    "    # python step1_audit.py\n",
    "    # Or specify different paths:\n",
    "    # python step1_audit.py /path/to/features.csv /tmp/out\n",
    "    import sys\n",
    "\n",
    "    features = \"features.csv\"\n",
    "    out = '.'\n",
    "    run_step1_audit(features, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8849c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 2: Aggregation Summary ===\n",
      "Clients: 59\n",
      "Output columns: 80\n",
      "Saved: aggregated_clients.csv, aggregated_feature_columns.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def parse_month_to_ym(m):\n",
    "    \"\"\"Parse to 'YYYY-MM' for counting distinct months; return None if unknown.\"\"\"\n",
    "    if pd.isna(m):\n",
    "        return None\n",
    "    dt = pd.to_datetime(str(m), errors=\"coerce\")\n",
    "    if pd.isna(dt):\n",
    "        return None\n",
    "    return f\"{dt.year:04d}-{dt.month:02d}\"\n",
    "\n",
    "\n",
    "def run_step2_aggregate(features_path: str, out_dir: str = \".\",\n",
    "                        aggregated_filename: str = \"aggregated_clients.csv\",\n",
    "                        schema_filename: str = \"aggregated_feature_columns.json\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads features.csv with monthly rows and returns a per-client aggregated dataframe.\n",
    "\n",
    "    Outputs:\n",
    "      - aggregated_clients.csv  (one row per client)\n",
    "      - aggregated_feature_columns.json  (ordered list of numeric feature columns for modeling)\n",
    "    \"\"\"\n",
    "    features_path = Path(features_path)\n",
    "    out_dir = Path(out_dir)\n",
    "    assert features_path.exists(), f\"File not found: {features_path}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(features_path)\n",
    "\n",
    "    # ---- Identify numeric columns\n",
    "    trs_cols = [c for c in df.columns if c.startswith(\"trs_\")]\n",
    "    trf_cols = [c for c in df.columns if c.startswith(\"trf_\")]\n",
    "    base_nums = [c for c in [\"income_total\", \"spending_total\", \"avg_monthly_balance_KZT\", \"age\"] if c in df.columns]\n",
    "\n",
    "    # Coerce numerics and fill NaN with 0 (for flows); age we'll handle separately\n",
    "    num_to_fill = sorted(set(trs_cols + trf_cols + [c for c in base_nums if c != \"age\"]))\n",
    "    for c in num_to_fill:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "    if \"age\" in df.columns:\n",
    "        df[\"age\"] = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n",
    "\n",
    "    # Parse month for counting\n",
    "    if \"month\" in df.columns:\n",
    "        df[\"_month_ym\"] = df[\"month\"].apply(parse_month_to_ym)\n",
    "    else:\n",
    "        df[\"_month_ym\"] = None\n",
    "\n",
    "    # ---- Aggregate to one row per client\n",
    "    rows = []\n",
    "    for client, g in df.groupby(\"client_code\"):\n",
    "        row = {\"client_code\": client}\n",
    "\n",
    "        # Keep metadata (first non-null)\n",
    "        for col in [\"name\", \"status\", \"city\"]:\n",
    "            if col in g.columns:\n",
    "                vals = g[col].dropna()\n",
    "                row[col] = vals.iloc[0] if not vals.empty else None\n",
    "\n",
    "        # Age: take first non-null\n",
    "        if \"age\" in g.columns:\n",
    "            vals = g[\"age\"].dropna()\n",
    "            row[\"age\"] = float(vals.iloc[0]) if not vals.empty else None\n",
    "\n",
    "        # Months observed\n",
    "        n_months = g[\"_month_ym\"].nunique()\n",
    "        row[\"n_months\"] = int(n_months)\n",
    "\n",
    "        # Balances: average across months; totals: sum across months\n",
    "        if \"avg_monthly_balance_KZT\" in g.columns:\n",
    "            row[\"avg_monthly_balance_KZT\"] = float(g[\"avg_monthly_balance_KZT\"].mean())\n",
    "        row[\"income_total_3m\"] = float(g[\"income_total\"].sum()) if \"income_total\" in g.columns else 0.0\n",
    "        row[\"spending_total_3m\"] = float(g[\"spending_total\"].sum()) if \"spending_total\" in g.columns else 0.0\n",
    "\n",
    "        # Sum all trs_/trf_ across months\n",
    "        for c in trs_cols + trf_cols:\n",
    "            row[c] = float(g[c].sum())\n",
    "\n",
    "        # ---- Derived aggregates useful for downstream scoring\n",
    "        # Withdrawals / cash-like outflows\n",
    "        atm = row.get(\"trf_atm_withdrawal\", 0.0)\n",
    "        card_out = row.get(\"trf_card_out\", 0.0)\n",
    "        withdrawals_total = atm + card_out\n",
    "        row[\"withdrawals_total\"] = withdrawals_total\n",
    "        row[\"withdrawals_to_spend_ratio\"] = (\n",
    "            withdrawals_total / row[\"spending_total_3m\"] if row[\"spending_total_3m\"] > 0 else 0.0\n",
    "        )\n",
    "\n",
    "        # FX / Invest / Deposits\n",
    "        row[\"fx_turnover\"] = row.get(\"trf_fx_buy\", 0.0) + row.get(\"trf_fx_sell\", 0.0)\n",
    "        row[\"invest_turnover\"] = row.get(\"trf_invest_in\", 0.0) + row.get(\"trf_invest_out\", 0.0)\n",
    "        row[\"deposit_topups\"] = row.get(\"trf_deposit_topup_out\", 0.0) + row.get(\"trf_deposit_fx_topup_out\", 0.0)\n",
    "        row[\"cc_repayments\"] = row.get(\"trf_cc_repayment_out\", 0.0)\n",
    "        row[\"loan_payments\"] = row.get(\"trf_loan_payment_out\", 0.0)\n",
    "\n",
    "        # Product-oriented spend clusters\n",
    "        TRAVEL = [\"trs_Путешествия\", \"trs_Такси\", \"trs_Отели\"]\n",
    "        ONLINE = [\"trs_Играем дома\", \"trs_Смотрим дома\", \"trs_Едим дома\"]\n",
    "        PREMIUM_BOOST = [\"trs_Ювелирные украшения\", \"trs_Косметика и Парфюмерия\", \"trs_Кафе и рестораны\"]\n",
    "\n",
    "        row[\"travel_spend\"] = float(sum(row.get(c, 0.0) for c in TRAVEL))\n",
    "        row[\"online_spend\"] = float(sum(row.get(c, 0.0) for c in ONLINE))\n",
    "        row[\"premium_boost_spend\"] = float(sum(row.get(c, 0.0) for c in PREMIUM_BOOST))\n",
    "\n",
    "        # Shares per spending category\n",
    "        spend = row[\"spending_total_3m\"]\n",
    "        for c in trs_cols:\n",
    "            share_col = \"share_\" + c.replace(\"trs_\", \"\")\n",
    "            row[share_col] = (row.get(c, 0.0) / spend) if spend > 0 else 0.0\n",
    "\n",
    "        # Top-3 spending categories (names without 'trs_')\n",
    "        cat_pairs = [(c.replace(\"trs_\", \"\"), row.get(c, 0.0)) for c in trs_cols]\n",
    "        cat_pairs = [(n, v) for n, v in cat_pairs if v and v > 0]\n",
    "        cat_pairs.sort(key=lambda kv: kv[1], reverse=True)\n",
    "        row[\"top_cat1\"] = cat_pairs[0][0] if len(cat_pairs) > 0 else None\n",
    "        row[\"top_cat2\"] = cat_pairs[1][0] if len(cat_pairs) > 1 else None\n",
    "        row[\"top_cat3\"] = cat_pairs[2][0] if len(cat_pairs) > 2 else None\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    agg = pd.DataFrame(rows)\n",
    "\n",
    "    # ---- Column ordering: id/meta, core totals, derived, raw sums, shares, tops\n",
    "    id_meta = [c for c in [\"client_code\", \"name\", \"status\", \"city\", \"age\", \"n_months\"] if c in agg.columns]\n",
    "    core = [c for c in [\"avg_monthly_balance_KZT\", \"income_total_3m\", \"spending_total_3m\"] if c in agg.columns]\n",
    "    derived = [\n",
    "        \"fx_turnover\", \"invest_turnover\", \"withdrawals_total\", \"withdrawals_to_spend_ratio\",\n",
    "        \"deposit_topups\", \"cc_repayments\", \"loan_payments\",\n",
    "        \"travel_spend\", \"online_spend\", \"premium_boost_spend\"\n",
    "    ]\n",
    "    raw_sums = trs_cols + trf_cols\n",
    "    share_cols = [c for c in agg.columns if c.startswith(\"share_\")]\n",
    "    tops = [\"top_cat1\", \"top_cat2\", \"top_cat3\"]\n",
    "\n",
    "    ordered_cols = id_meta + core + derived + raw_sums + share_cols + tops\n",
    "    # Keep only columns that exist\n",
    "    ordered_cols = [c for c in ordered_cols if c in agg.columns]\n",
    "    agg = agg[ordered_cols]\n",
    "\n",
    "    # ---- Save outputs\n",
    "    out_csv = out_dir / aggregated_filename\n",
    "    agg.to_csv(out_csv, index=False)\n",
    "\n",
    "    feature_cols = core + derived + raw_sums + share_cols  # numeric set for modeling later\n",
    "    schema = {\n",
    "        \"feature_columns\": feature_cols,\n",
    "        \"notes\": [\n",
    "            \"Do NOT use 'name/city/status' as model inputs unless explicitly desired.\",\n",
    "            \"We ignore the original 'product' column from features.csv for labels; labels will come from the EV-teacher.\",\n",
    "        ],\n",
    "    }\n",
    "    with open(out_dir / schema_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(schema, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Console summary\n",
    "    print(\"=== STEP 2: Aggregation Summary ===\")\n",
    "    print(f\"Clients: {agg.shape[0]}\")\n",
    "    print(f\"Output columns: {agg.shape[1]}\")\n",
    "    print(f\"Saved: {out_csv.name}, {schema_filename}\")\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Usage:\n",
    "    #   python step2_aggregate.py /mnt/data/features.csv /mnt/data\n",
    "    import sys\n",
    "    features = \"features.csv\"\n",
    "    out = \".\"\n",
    "    run_step2_aggregate(features, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66e267ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 3: EV Teacher Summary ===\n",
      "Saved: ev_scores.csv, ev_labels.csv, ev_teacher_config.json\n",
      "Winners distribution:\n",
      " - Кредитная карта: 47\n",
      " - Премиальная карта: 12\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "PRODUCTS = [\n",
    "    \"Карта для путешествий\",\n",
    "    \"Премиальная карта\",\n",
    "    \"Кредитная карта\",\n",
    "    \"Обмен валют\",\n",
    "    \"Кредит наличными\",\n",
    "    \"Депозит Мультивалютный\",\n",
    "    \"Депозит Сберегательный\",\n",
    "    \"Депозит Накопительный\",\n",
    "    \"Инвестиции\",\n",
    "    \"Золотые слитки\",\n",
    "]\n",
    "\n",
    "CFG = {\n",
    "    # Cashback / interest parameters\n",
    "    \"travel_cb_rate\": 0.04,                     # 4% on travel+taxi+hotels\n",
    "    \"premium_base_thresholds\": [                # (min_balance_or_topups, base_rate)\n",
    "        (6_000_000, 0.04),\n",
    "        (1_000_000, 0.03),\n",
    "        (0,         0.02),\n",
    "    ],\n",
    "    \"premium_boost_rate\": 0.04,                 # boosted categories up to 4%\n",
    "    \"premium_monthly_cap\": 100_000,             # per month cap\n",
    "    \"credit_card_rate\": 0.10,                   # top-3 categories + online services\n",
    "    \"fx_saving_rate\": 0.003,                    # 0.3% of FX turnover\n",
    "    \"dep_rates\": {                              # annual nominal\n",
    "        \"Депозит Сберегательный\": 0.165,\n",
    "        \"Депозит Накопительный\": 0.155,\n",
    "        \"Депозит Мультивалютный\": 0.145,\n",
    "    },\n",
    "    \"invest_saving_rate\": 0.002,                # 0.2% of invest turnover (proxy for fee save)\n",
    "    \"gold_saving_rate\": 0.001,                  # 0.1% of gold turnover (very conservative)\n",
    "    # Gating/penalties (apply if behavioral signals are weak)\n",
    "    \"dep_penalties\": {\n",
    "        # multipliers applied if condition is NOT met\n",
    "        \"Депозит Сберегательный\": 0.30,         # needs stable large balance & low withdrawals\n",
    "        \"Депозит Накопительный\": 0.50,          # prefers positive cashflow (income > spend)\n",
    "        \"Депозит Мультивалютный\": 0.70,         # prefers FX activity\n",
    "    },\n",
    "    \"loan_ev_base\": 1.0,                        # tiny EV so loan wins only with strong need\n",
    "    \"loan_need\": {                              \n",
    "        \"low_balance_threshold\": 300_000,\n",
    "        \"outflow_multiplier\": 1.10,             # spend > 1.1 * income\n",
    "    },\n",
    "}\n",
    "\n",
    "# Helper: safe getter\n",
    "def g(row, key, default=0.0):\n",
    "    v = row.get(key, default)\n",
    "    try:\n",
    "        return float(v) if v is not None else float(default)\n",
    "    except Exception:\n",
    "        return float(default)\n",
    "\n",
    "def months_frac(n_months: float) -> float:\n",
    "    n = max(1.0, float(n_months) if pd.notna(n_months) else 3.0)\n",
    "    return n / 12.0\n",
    "\n",
    "def top3_spending_categories(row: dict) -> List[Tuple[str, float]]:\n",
    "    # find aggregated trs_* columns\n",
    "    entries = [(k, g(row, k)) for k in row.keys() if k.startswith(\"trs_\")]\n",
    "    entries = [(k, v) for k, v in entries if v > 0]\n",
    "    entries.sort(key=lambda kv: kv[1], reverse=True)\n",
    "    return entries[:3]  # [(col_name, spend), ...]\n",
    "\n",
    "def ev_travel(row: dict) -> float:\n",
    "    travel_spend = g(row, \"trs_Путешествия\") + g(row, \"trs_Такси\") + g(row, \"trs_Отели\")\n",
    "    return CFG[\"travel_cb_rate\"] * travel_spend\n",
    "\n",
    "def ev_premium(row: dict) -> float:\n",
    "    # base rate by avg balance OR deposit topups\n",
    "    avg_bal = g(row, \"avg_monthly_balance_KZT\")\n",
    "    dep_topups = g(row, \"trf_deposit_topup_out\") + g(row, \"trf_deposit_fx_topup_out\")\n",
    "    base_rate = 0.02\n",
    "    for thr, rate in CFG[\"premium_base_thresholds\"]:\n",
    "        if avg_bal >= thr or dep_topups >= thr:\n",
    "            base_rate = rate\n",
    "            break\n",
    "\n",
    "    n_months = max(1.0, g(row, \"n_months\", 3.0))\n",
    "    spend_3m = g(row, \"spending_total_3m\")\n",
    "    spend_month = spend_3m / n_months\n",
    "\n",
    "    # boosted categories per month\n",
    "    booster_3m = g(row, \"trs_Ювелирные украшения\") + g(row, \"trs_Косметика и Парфюмерия\") + g(row, \"trs_Кафе и рестораны\")\n",
    "    booster_month = booster_3m / n_months\n",
    "    extra_rate = max(0.0, CFG[\"premium_boost_rate\"] - base_rate)\n",
    "    cb_month = base_rate * spend_month + extra_rate * booster_month\n",
    "    cb_month = min(cb_month, CFG[\"premium_monthly_cap\"])\n",
    "    return cb_month * n_months\n",
    "\n",
    "def ev_credit_card(row: dict) -> float:\n",
    "    entries = top3_spending_categories(row)  # list of (trs_col, spend)\n",
    "    top3_sum = sum(v for _, v in entries)\n",
    "    # online services (exclude if already in top-3 to avoid double-count)\n",
    "    online_cols = [\"trs_Играем дома\", \"trs_Смотрим дома\", \"trs_Едим дома\"]\n",
    "    online_extra = 0.0\n",
    "    top3_set = {k for k, _ in entries}\n",
    "    for c in online_cols:\n",
    "        if c not in top3_set:\n",
    "            online_extra += g(row, c)\n",
    "    eligible = top3_sum + online_extra\n",
    "    return CFG[\"credit_card_rate\"] * eligible\n",
    "\n",
    "def ev_fx(row: dict) -> float:\n",
    "    fx_turnover = g(row, \"trf_fx_buy\") + g(row, \"trf_fx_sell\")\n",
    "    return CFG[\"fx_saving_rate\"] * fx_turnover\n",
    "\n",
    "def ev_deposit(row: dict, dep_name: str) -> float:\n",
    "    rate = CFG[\"dep_rates\"][dep_name]\n",
    "    n_frac = months_frac(g(row, \"n_months\", 3.0))\n",
    "    base_ev = rate * g(row, \"avg_monthly_balance_KZT\") * n_frac\n",
    "\n",
    "    # gating penalties if behavior not aligned\n",
    "    penalty = 1.0\n",
    "    if dep_name == \"Депозит Сберегательный\":\n",
    "        stable_enough = (g(row, \"avg_monthly_balance_KZT\") >= 2_000_000) and (g(row, \"withdrawals_to_spend_ratio\") < 0.20)\n",
    "        if not stable_enough:\n",
    "            penalty = CFG[\"dep_penalties\"][dep_name]\n",
    "    elif dep_name == \"Депозит Накопительный\":\n",
    "        positive_net = g(row, \"income_total_3m\") > g(row, \"spending_total_3m\")\n",
    "        if not positive_net:\n",
    "            penalty = CFG[\"dep_penalties\"][dep_name]\n",
    "    elif dep_name == \"Депозит Мультивалютный\":\n",
    "        fx_active = (g(row, \"trf_fx_buy\") + g(row, \"trf_fx_sell\")) > 0\n",
    "        if not fx_active:\n",
    "            penalty = CFG[\"dep_penalties\"][dep_name]\n",
    "\n",
    "    return base_ev * penalty\n",
    "\n",
    "def ev_invest(row: dict) -> float:\n",
    "    invest_turnover = g(row, \"trf_invest_in\") + g(row, \"trf_invest_out\")\n",
    "    return CFG[\"invest_saving_rate\"] * invest_turnover\n",
    "\n",
    "def ev_gold(row: dict) -> float:\n",
    "    # If transfers exist as trf_gold_buy_out / trf_gold_sell_in, use them; else fall back to 0.\n",
    "    gold_buy = g(row, \"trf_gold_buy_out\")\n",
    "    gold_sell = g(row, \"trf_gold_sell_in\")\n",
    "    gold_turnover = gold_buy + gold_sell\n",
    "    return CFG[\"gold_saving_rate\"] * gold_turnover\n",
    "\n",
    "def ev_cash_loan(row: dict) -> float:\n",
    "    # Do not \"sell\" unless liquidity stress is visible\n",
    "    low_balance = g(row, \"avg_monthly_balance_KZT\") < CFG[\"loan_need\"][\"low_balance_threshold\"]\n",
    "    repayments = g(row, \"trf_loan_payment_out\") + g(row, \"trf_cc_repayment_out\")\n",
    "    income = g(row, \"income_total_3m\")\n",
    "    spend = g(row, \"spending_total_3m\")\n",
    "    large_outflows = (income > 0) and (spend > CFG[\"loan_need\"][\"outflow_multiplier\"] * income)\n",
    "    need = (repayments > 0 and low_balance) or large_outflows\n",
    "    return CFG[\"loan_ev_base\"] if need else 0.0\n",
    "\n",
    "\n",
    "def score_row(row: dict) -> Dict[str, float]:\n",
    "    ev = {}\n",
    "    ev[\"Карта для путешествий\"] = ev_travel(row)\n",
    "    ev[\"Премиальная карта\"]     = ev_premium(row)\n",
    "    ev[\"Кредитная карта\"]       = ev_credit_card(row)\n",
    "    ev[\"Обмен валют\"]           = ev_fx(row)\n",
    "    ev[\"Кредит наличными\"]      = ev_cash_loan(row)\n",
    "    ev[\"Депозит Мультивалютный\"]= ev_deposit(row, \"Депозит Мультивалютный\")\n",
    "    ev[\"Депозит Сберегательный\"]= ev_deposit(row, \"Депозит Сберегательный\")\n",
    "    ev[\"Депозит Накопительный\"] = ev_deposit(row, \"Депозит Накопительный\")\n",
    "    ev[\"Инвестиции\"]            = ev_invest(row)\n",
    "    ev[\"Золотые слитки\"]        = ev_gold(row)\n",
    "    return ev\n",
    "\n",
    "\n",
    "def run_step3_ev_teacher(aggregated_path: str, out_dir: str = \".\",\n",
    "                         ev_scores_filename=\"ev_scores.csv\",\n",
    "                         ev_labels_filename=\"ev_labels.csv\",\n",
    "                         config_filename=\"ev_teacher_config.json\") -> None:\n",
    "    aggregated_path = Path(aggregated_path)\n",
    "    out_dir = Path(out_dir)\n",
    "    assert aggregated_path.exists(), f\"File not found: {aggregated_path}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    agg = pd.read_csv(aggregated_path)\n",
    "    # ensure numeric where needed\n",
    "    for c in agg.columns:\n",
    "        if c.startswith((\"trs_\", \"trf_\")) or c in [\n",
    "            \"avg_monthly_balance_KZT\", \"spending_total_3m\", \"income_total_3m\",\n",
    "            \"withdrawals_to_spend_ratio\", \"withdrawals_total\",\n",
    "            \"fx_turnover\", \"invest_turnover\", \"deposit_topups\",\n",
    "            \"cc_repayments\", \"loan_payments\", \"n_months\", \"age\"\n",
    "        ]:\n",
    "            agg[c] = pd.to_numeric(agg[c], errors=\"coerce\")\n",
    "\n",
    "    # compute EV table\n",
    "    ev_rows: List[Dict[str, float]] = []\n",
    "    label_rows: List[Dict[str, object]] = []\n",
    "\n",
    "    for _, r in agg.iterrows():\n",
    "        row = r.to_dict()\n",
    "        ev = score_row(row)  # dict product -> EV\n",
    "        # EV soft labels row\n",
    "        ev_row = {\"client_code\": r[\"client_code\"]}\n",
    "        ev_row.update({p: float(ev.get(p, 0.0)) for p in PRODUCTS})\n",
    "        ev_rows.append(ev_row)\n",
    "\n",
    "        # hard label (argmax) + Top-4\n",
    "        sorted_items = sorted(ev.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        top4 = sorted_items[:4]\n",
    "        label_rows.append({\n",
    "            \"client_code\": r[\"client_code\"],\n",
    "            \"label_best\": top4[0][0],\n",
    "            \"ev_best\": float(top4[0][1]),\n",
    "            \"label_2\": top4[1][0], \"ev_2\": float(top4[1][1]),\n",
    "            \"label_3\": top4[2][0], \"ev_3\": float(top4[2][1]),\n",
    "            \"label_4\": top4[3][0], \"ev_4\": float(top4[3][1]),\n",
    "        })\n",
    "\n",
    "    ev_df = pd.DataFrame(ev_rows, columns=[\"client_code\"] + PRODUCTS)\n",
    "    labels_df = pd.DataFrame(label_rows)\n",
    "\n",
    "    ev_path = out_dir / ev_scores_filename\n",
    "    labels_path = out_dir / ev_labels_filename\n",
    "    ev_df.to_csv(ev_path, index=False)\n",
    "    labels_df.to_csv(labels_path, index=False)\n",
    "\n",
    "    # Save config for reproducibility\n",
    "    cfg_out = {\n",
    "        \"products\": PRODUCTS,\n",
    "        \"config\": CFG,\n",
    "        \"notes\": [\n",
    "            \"EV are 3-month expected gains (cashback/interest/savings).\",\n",
    "            \"Deposit EVs are gated by behavioral signals via penalties (see dep_penalties).\",\n",
    "            \"Loan EV is tiny unless liquidity stress signals are present.\",\n",
    "            \"Gold EV is conservative and only activates with gold turnover in transfers.\",\n",
    "        ],\n",
    "        \"inputs_expected\": [\n",
    "            \"avg_monthly_balance_KZT\", \"spending_total_3m\", \"income_total_3m\", \"n_months\",\n",
    "            \"withdrawals_to_spend_ratio\", \"trf_*\", \"trs_*\"\n",
    "        ],\n",
    "        \"columns_optional\": [\"trf_gold_buy_out\", \"trf_gold_sell_in\"],  # only if present in your data\n",
    "    }\n",
    "    with open(out_dir / config_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cfg_out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"=== STEP 3: EV Teacher Summary ===\")\n",
    "    print(f\"Saved: {ev_path.name}, {labels_path.name}, {config_filename}\")\n",
    "    # quick sanity: show how many times each product wins\n",
    "    winners = labels_df[\"label_best\"].value_counts().sort_values(ascending=False)\n",
    "    print(\"Winners distribution:\")\n",
    "    for p, cnt in winners.items():\n",
    "        print(f\" - {p}: {int(cnt)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Usage:\n",
    "    #   python step3_ev_teacher.py /mnt/data/aggregated_clients.csv /mnt/data\n",
    "    import sys\n",
    "    agg_path = \"aggregated_clients.csv\"\n",
    "    out_dir = \".\"\n",
    "    run_step3_ev_teacher(agg_path, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "257caff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 4: Classifier Training Summary ===\n",
      "Splits: 3\n",
      "Top-1 mean: 0.744 | Top-3 mean: 1.000 | EV-regret mean (₸): 5496.4\n",
      "Saved: reco_model.pkl, reco_eval.csv, reco_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# step4_train_classifier.py\n",
    "# STEP 4 — Train a baseline classifier from EV-teacher labels (FIXED).\n",
    "#\n",
    "# Inputs:\n",
    "#   - aggregated_clients.csv            (from step 2)\n",
    "#   - aggregated_feature_columns.json   (from step 2; optional but preferred)\n",
    "#   - ev_labels.csv                     (from step 3; hard labels)\n",
    "#   - ev_scores.csv                     (from step 3; soft labels / EV per product)\n",
    "#\n",
    "# Outputs:\n",
    "#   - reco_model.pkl                    (scikit-learn model + metadata)\n",
    "#   - reco_eval.csv                     (CV metrics per fold: top1, top3, EV-regret)\n",
    "#   - reco_predictions.csv              (per-client out-of-fold predictions w/ EV regrets)\n",
    "#\n",
    "# Notes:\n",
    "#   - Handles folds where the training set has only 1 class (falls back to DummyClassifier).\n",
    "#   - Maps fold-local predict_proba to the GLOBAL class space before metrics (fixes label mismatch).\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "# -------------------- Utils --------------------\n",
    "META_COLS = [\n",
    "    \"client_code\", \"name\", \"status\", \"city\", \"age\", \"n_months\",\n",
    "    \"top_cat1\", \"top_cat2\", \"top_cat3\"\n",
    "]\n",
    "\n",
    "def load_feature_list(schema_path: Path, agg_cols: List[str]) -> List[str]:\n",
    "    \"\"\"Load preferred feature columns; fall back to numeric-like columns if schema missing.\"\"\"\n",
    "    if schema_path.exists():\n",
    "        with open(schema_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            schema = json.load(f)\n",
    "        feats = [c for c in schema.get(\"feature_columns\", []) if c in agg_cols]\n",
    "        if feats:\n",
    "            return feats\n",
    "\n",
    "    numeric_like = []\n",
    "    for c in agg_cols:\n",
    "        if c in META_COLS:\n",
    "            continue\n",
    "        if c.startswith((\"trs_\", \"trf_\", \"share_\")) or c in [\n",
    "            \"avg_monthly_balance_KZT\", \"income_total_3m\", \"spending_total_3m\",\n",
    "            \"fx_turnover\", \"invest_turnover\", \"withdrawals_total\", \"withdrawals_to_spend_ratio\",\n",
    "            \"deposit_topups\", \"cc_repayments\", \"loan_payments\",\n",
    "            \"travel_spend\", \"online_spend\", \"premium_boost_spend\"\n",
    "        ]:\n",
    "            numeric_like.append(c)\n",
    "    return numeric_like\n",
    "\n",
    "\n",
    "# -------------------- Main --------------------\n",
    "def run_step4_train(\n",
    "    aggregated_path: str,\n",
    "    ev_labels_path: str,\n",
    "    ev_scores_path: str,\n",
    "    out_dir: str = \".\",\n",
    "    schema_path: str = \"aggregated_feature_columns.json\",\n",
    "    model_filename: str = \"reco_model.pkl\",\n",
    "    eval_filename: str = \"reco_eval.csv\",\n",
    "    preds_filename: str = \"reco_predictions.csv\",\n",
    "):\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    agg_path = Path(aggregated_path)\n",
    "    labels_path = Path(ev_labels_path)\n",
    "    scores_path = Path(ev_scores_path)\n",
    "    schema_path = Path(schema_path)\n",
    "\n",
    "    assert agg_path.exists(), f\"Not found: {agg_path}\"\n",
    "    assert labels_path.exists(), f\"Not found: {labels_path}\"\n",
    "    assert scores_path.exists(), f\"Not found: {scores_path}\"\n",
    "\n",
    "    # Load data\n",
    "    agg = pd.read_csv(agg_path)\n",
    "    labels = pd.read_csv(labels_path)   # client_code, label_best, ev_best, label_2..label_4...\n",
    "    scores = pd.read_csv(scores_path)   # client_code + EV per product\n",
    "\n",
    "    # Global product classes from ev_scores columns (order is fixed and global)\n",
    "    classes = [c for c in scores.columns if c != \"client_code\"]\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "\n",
    "    # Merge features + labels + scores\n",
    "    df = agg.merge(labels[[\"client_code\", \"label_best\", \"ev_best\"]], on=\"client_code\", how=\"inner\")\n",
    "    df = df.merge(scores, on=\"client_code\", how=\"inner\", suffixes=(\"\", \"\"))\n",
    "\n",
    "    # Feature columns\n",
    "    feature_cols = load_feature_list(schema_path, df.columns.tolist())\n",
    "    if not feature_cols:\n",
    "        raise RuntimeError(\"No feature columns selected. Check aggregated_feature_columns.json or your input schema.\")\n",
    "    for c in feature_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Target indices\n",
    "    y_labels = df[\"label_best\"].astype(str)\n",
    "    y_idx = y_labels.map(class_to_idx)\n",
    "    mask = y_idx.notna()\n",
    "    df = df[mask].reset_index(drop=True)\n",
    "    y_idx = y_idx[mask].astype(int)\n",
    "\n",
    "    groups = df[\"client_code\"].values\n",
    "    X = df[feature_cols].copy()\n",
    "\n",
    "    # Base estimator\n",
    "    base_clf = HistGradientBoostingClassifier(\n",
    "        max_depth=4,\n",
    "        learning_rate=0.10,\n",
    "        max_iter=300,\n",
    "        l2_regularization=0.05,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # CV splits\n",
    "    n = len(df)\n",
    "    n_splits = min(5, max(3, n // 15))  # heuristic for small data\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    records = []\n",
    "    fold_preds = []\n",
    "\n",
    "    for fold, (tr, te) in enumerate(gkf.split(X, y_idx, groups=groups), start=1):\n",
    "        Xtr, Xte = X.iloc[tr], X.iloc[te]\n",
    "        ytr, yte = y_idx.iloc[tr], y_idx.iloc[te]\n",
    "        te_idx = df.index[te]\n",
    "\n",
    "        # Fallback if only one class present in the training fold\n",
    "        unique_train = np.unique(ytr)\n",
    "        if len(unique_train) < 2:\n",
    "            clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "        else:\n",
    "            clf = base_clf\n",
    "\n",
    "        clf.fit(Xtr, ytr)\n",
    "\n",
    "        # --- Map fold-local proba to GLOBAL class space ---\n",
    "        # Some estimators don't have predict_proba (HGB does). For Dummy, it does as well.\n",
    "        proba_local = clf.predict_proba(Xte)\n",
    "        local_classes = clf.classes_.astype(int)              # indices in GLOBAL classes\n",
    "        proba_full = np.zeros((Xte.shape[0], len(classes)))   # (n_test, n_global_classes)\n",
    "        proba_full[:, local_classes] = proba_local            # place local probs into global columns\n",
    "\n",
    "        # Global predictions\n",
    "        yhat_global = proba_full.argmax(axis=1)\n",
    "\n",
    "        # Metrics\n",
    "        acc = accuracy_score(yte, yhat_global)\n",
    "        top3 = top_k_accuracy_score(\n",
    "            yte,\n",
    "            proba_full,\n",
    "            k=3,\n",
    "            labels=list(range(len(classes)))  # [0..len-1], matches proba_full columns\n",
    "        )\n",
    "\n",
    "        # EV-regret\n",
    "        ev_best = df.loc[te_idx, \"ev_best\"].values.astype(float)\n",
    "        ev_pred = []\n",
    "        for i, row_i in enumerate(te_idx):\n",
    "            pred_class = classes[yhat_global[i]]              # global class -> product name\n",
    "            ev_pred.append(float(df.loc[row_i, pred_class]))\n",
    "        ev_pred = np.array(ev_pred, dtype=float)\n",
    "        ev_regret = float((ev_best - ev_pred).mean())\n",
    "\n",
    "        records.append({\n",
    "            \"fold\": fold,\n",
    "            \"top1_acc\": float(acc),\n",
    "            \"top3_acc\": float(top3),\n",
    "            \"mean_ev_regret\": ev_regret,\n",
    "            \"n_test\": int(len(te_idx)),\n",
    "        })\n",
    "\n",
    "        # Save out-of-fold predictions\n",
    "        fold_pred_df = pd.DataFrame({\n",
    "            \"client_code\": df.loc[te_idx, \"client_code\"].values,\n",
    "            \"label_best_teacher\": df.loc[te_idx, \"label_best\"].values,\n",
    "            \"recommended_product\": [classes[i] for i in yhat_global],\n",
    "            \"ev_best_teacher\": ev_best,\n",
    "            \"ev_predicted\": ev_pred,\n",
    "            \"ev_regret\": ev_best - ev_pred,\n",
    "        })\n",
    "        fold_preds.append(fold_pred_df)\n",
    "\n",
    "    eval_df = pd.DataFrame(records)\n",
    "    eval_df.to_csv(out_dir / eval_filename, index=False)\n",
    "\n",
    "    # Train final model on ALL data (fallback to Dummy if only 1 class globally)\n",
    "    if len(np.unique(y_idx)) < 2:\n",
    "        final_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "    else:\n",
    "        final_clf = base_clf\n",
    "    final_clf.fit(X, y_idx)\n",
    "\n",
    "    # Save artifacts\n",
    "    model_pack = {\n",
    "        \"model\": final_clf,\n",
    "        \"classes\": classes,           # ordered product names\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"metadata\": {\n",
    "            \"n_clients\": int(len(df)),\n",
    "            \"n_features\": int(len(feature_cols)),\n",
    "            \"products\": classes,\n",
    "            \"cv_summary\": {\n",
    "                \"top1_acc_mean\": float(eval_df[\"top1_acc\"].mean()),\n",
    "                \"top3_acc_mean\": float(eval_df[\"top3_acc\"].mean()),\n",
    "                \"mean_ev_regret\": float(eval_df[\"mean_ev_regret\"].mean()),\n",
    "                \"n_splits\": int(n_splits),\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    joblib.dump(model_pack, out_dir / model_filename)\n",
    "\n",
    "    # Consolidated out-of-fold predictions\n",
    "    preds_df = pd.concat(fold_preds, ignore_index=True).sort_values(\"client_code\")\n",
    "    preds_df.to_csv(out_dir / preds_filename, index=False)\n",
    "\n",
    "    # Console summary\n",
    "    print(\"=== STEP 4: Classifier Training Summary ===\")\n",
    "    print(f\"Splits: {n_splits}\")\n",
    "    print(f\"Top-1 mean: {eval_df['top1_acc'].mean():.3f} | \"\n",
    "          f\"Top-3 mean: {eval_df['top3_acc'].mean():.3f} | \"\n",
    "          f\"EV-regret mean (₸): {eval_df['mean_ev_regret'].mean():.1f}\")\n",
    "    print(f\"Saved: {model_filename}, {eval_filename}, {preds_filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Usage:\n",
    "    #   python step4_train_classifier.py /path/aggregated_clients.csv /path/ev_labels.csv /path/ev_scores.csv /out/dir\n",
    "    import sys\n",
    "    agg = \"aggregated_clients.csv\"\n",
    "    evl = \"ev_labels.csv\"\n",
    "    evs = \"ev_scores.csv\"\n",
    "    out = \".\"\n",
    "    run_step4_train(agg, evl, evs, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb68c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step5_infer.py\n",
    "# STEP 5 — Inference stub & Top-N API.\n",
    "#\n",
    "# What it does:\n",
    "#   • Loads reco_model.pkl (from step 4)\n",
    "#   • Accepts an aggregated client CSV (from step 2) OR a single JSON row\n",
    "#   • Aligns features, maps fold-local classes → global classes\n",
    "#   • Returns Top-N recommendations per client (+ per-class probabilities)\n",
    "#\n",
    "# Outputs:\n",
    "#   - reco_inference.csv  (client_code, top1, top1_conf, topN list, per-class probs)\n",
    "#\n",
    "# Usage examples:\n",
    "#   python step5_infer.py /mnt/data/aggregated_clients.csv /mnt/data --topk 3\n",
    "#   # Or read a single-row JSON:\n",
    "#   echo '{\"client_code\":999, \"avg_monthly_balance_KZT\":1200000, ...}' | \\\n",
    "#     python step5_infer.py - - --topk 5\n",
    "#\n",
    "# Notes:\n",
    "#   - This uses ONLY the trained classifier. Blending with EV-teacher is step 6+ (optional).\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "\n",
    "def load_model(model_path: str):\n",
    "    pack = joblib.load(model_path)\n",
    "    model = pack[\"model\"]\n",
    "    classes = pack[\"classes\"]           # global list of product names (ordered)\n",
    "    feature_cols = pack[\"feature_cols\"] # ordered features expected by the model\n",
    "    meta = pack.get(\"metadata\", {})\n",
    "    return model, classes, feature_cols, meta\n",
    "\n",
    "\n",
    "def ensure_dataframe(input_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    If input_path == '-', read one JSON object from stdin and make a 1-row DataFrame.\n",
    "    Else, read CSV.\n",
    "    \"\"\"\n",
    "    if input_path == \"-\":\n",
    "        raw = sys.stdin.read().strip()\n",
    "        row = json.loads(raw)\n",
    "        return pd.DataFrame([row])\n",
    "    p = Path(input_path)\n",
    "    assert p.exists(), f\"Input not found: {p}\"\n",
    "    return pd.read_csv(p)\n",
    "\n",
    "\n",
    "def align_features(df: pd.DataFrame, feature_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure all required columns exist; fill missing with 0.0; cast to float where possible.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    for c in feature_cols:\n",
    "        if c not in out.columns:\n",
    "            out[c] = 0.0\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out[feature_cols]\n",
    "\n",
    "\n",
    "def predict_topn(\n",
    "    model,\n",
    "    classes: List[str],\n",
    "    X: pd.DataFrame,\n",
    "    client_codes: np.ndarray,\n",
    "    topk: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute per-class probabilities mapped into the GLOBAL class space,\n",
    "    then produce Top-N recommendations per row.\n",
    "    \"\"\"\n",
    "    # Local probas → global map\n",
    "    proba_local = model.predict_proba(X)\n",
    "    local_classes_idx = model.classes_.astype(int)     # indices into GLOBAL classes\n",
    "    proba_full = np.zeros((X.shape[0], len(classes)), dtype=float)\n",
    "    proba_full[:, local_classes_idx] = proba_local\n",
    "\n",
    "    # Top-N\n",
    "    topk = int(max(1, min(topk, len(classes))))\n",
    "    order = np.argsort(-proba_full, axis=1)  # descending\n",
    "    top_indices = order[:, :topk]\n",
    "    top_scores = np.take_along_axis(proba_full, top_indices, axis=1)\n",
    "\n",
    "    # Build rows\n",
    "    rows = []\n",
    "    for i in range(X.shape[0]):\n",
    "        top_products = [classes[j] for j in top_indices[i]]\n",
    "        top_scores_i = [float(s) for s in top_scores[i]]\n",
    "        row = {\n",
    "            \"client_code\": client_codes[i] if \"client_code\" in df.columns else None,\n",
    "            \"top1_product\": top_products[0],\n",
    "            \"top1_confidence\": top_scores_i[0],\n",
    "            \"topN_products\": \"; \".join(top_products),\n",
    "            \"topN_confidences\": \"; \".join(f\"{s:.4f}\" for s in top_scores_i),\n",
    "        }\n",
    "        # Add per-class probability columns\n",
    "        for j, name in enumerate(classes):\n",
    "            row[f\"proba::{name}\"] = float(proba_full[i, j])\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run(\n",
    "    aggregated_path: str,\n",
    "    out_dir: str = \".\",\n",
    "    model_path: str = \"reco_model.pkl\",\n",
    "    topk: int = 3,\n",
    ") -> Path:\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model, classes, feature_cols, meta = load_model(model_path)\n",
    "\n",
    "    df_in = ensure_dataframe(aggregated_path)\n",
    "\n",
    "    # Keep client_code if present for output\n",
    "    global df\n",
    "    df = df_in.copy()\n",
    "\n",
    "    X = align_features(df_in, feature_cols)\n",
    "    client_codes = df_in[\"client_code\"].values if \"client_code\" in df_in.columns else np.arange(len(df_in))\n",
    "\n",
    "    pred_df = predict_topn(model, classes, X, client_codes, topk=topk)\n",
    "\n",
    "    out_path = out_dir / \"reco_inference.csv\"\n",
    "    pred_df.to_csv(out_path, index=False)\n",
    "\n",
    "    # Console summary\n",
    "    print(\"=== STEP 5: Inference Summary ===\")\n",
    "    print(f\"Rows inferred: {len(pred_df)} | topk={topk}\")\n",
    "    print(f\"Saved: {out_path.name}\")\n",
    "    if \"cv_summary\" in meta:\n",
    "        s = meta[\"cv_summary\"]\n",
    "        print(f\"Model CV — top1: {s.get('top1_acc_mean'):.3f}, top3: {s.get('top3_acc_mean'):.3f}, \"\n",
    "              f\"EV-regret: {s.get('mean_ev_regret'):.1f}, splits: {s.get('n_splits')}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # CLI\n",
    "    #   python step5_infer.py <aggregated_clients.csv|-> <out_dir> [--model reco_model.pkl] [--topk 3]\n",
    "    import argparse\n",
    "\n",
    "    ap = argparse.ArgumentParser(description=\"STEP 5 — Inference stub & Top-N API\")\n",
    "    ap.add_argument(\"input\", help=\"Aggregated CSV from step 2, or '-' to read one JSON row from stdin\")\n",
    "    ap.add_argument(\"out_dir\", help=\"Output directory for reco_inference.csv\")\n",
    "    ap.add_argument(\"--model\", default=\"reco_model.pkl\", help=\"Path to trained model .pkl\")\n",
    "    ap.add_argument(\"--topk\", type=int, default=3, help=\"How many top products to output\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    run(args.input, args.out_dir, args.model, args.topk)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
